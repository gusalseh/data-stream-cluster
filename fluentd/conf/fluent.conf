# file -> kafka
<source>
  @type tail
  path /fluentd/log/ai_data.json
  pos_file /fluentd/log/ai_data.pos
  tag ai.json
  format json
  read_from_head true  # 기존 파일 처음부터 처리
</source>

<filter ai.json>
  @type record_transformer
  enable_ruby true

  <record>
    topic ${if record["result"] == "positive" then "positive_topic" elsif record["result"] == "negative" then "negative_topic" else "others_topic" end}
  </record>
</filter>

<match ai.json>
  @type kafka
  brokers kafka:9092

  # topic_key로 레코드 내 'topic' 필드를 사용
  topic_key topic

  # 혹시 레코드에 'topic' 필드가 없을 경우 fallback
  default_topic others_topic

  output_data_type json
</match>

# kafka -> fluentd
<source>
  @type kafka_group
  brokers kafka:9092
  topics positive_topic, negative_topic, others_topic
  consumer_group fluentd-consumer-group
  format json
  # Kafka에서 읽은 메시지에 부여할 태그
  tag kafka.input
  # Kafka의 오프셋 관리를 위한 로컬 파일
  <storage>
    @type local
    path /fluentd/log/kafka-offsets.json
  </storage>
</source>


# [NEW] Kafka → OpenSearch
<match kafka.input>
  @type opensearch
  host opensearch
  port 9200
  scheme http

  # 인덱스 네이밍 - 날짜 별로 자동 구분(logstash_format)
  logstash_format true
  logstash_prefix from-kafka
  # record 내 timestamp 필드가 없다면 Fluentd가 자동 생성
  include_tag_key true
  type_name _doc
  suppress_type_name true
  reload_connections false
  reconnect_on_error true
  request_timeout 10s
  # 버퍼 설정
  <buffer>
    @type file
    path /fluentd/log/opensearch-buffer
    flush_interval 10s      # 데이터를 10초마다 전송
    retry_wait 5s           # 실패 시 5초 후에 재시도
    retry_forever true      # 무한 재시도
  </buffer>
</match>
